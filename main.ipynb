{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial code to test audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())  # Find virtual cable's index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "duration = 5  # seconds\n",
    "sample_rate = 16000  # Hz\n",
    "\n",
    "# Record audio\n",
    "print(\"Recording...\")\n",
    "audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2, dtype=np.float32)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording finished.\")\n",
    "\n",
    "# Play the recorded audio\n",
    "print(\"Playing back the recorded audio...\")\n",
    "sd.play(audio_data, samplerate=sample_rate)\n",
    "sd.wait()  # Wait until playback is finished\n",
    "print(\"Playback finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Playing back the recorded audio...\")\n",
    "sd.play(audio_data, samplerate=sample_rate)\n",
    "sd.wait()  # Wait until playback is finished\n",
    "print(\"Playback finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "from collections import deque\n",
    "from whispercpp import Whisper\n",
    "from openai import OpenAI\n",
    "import soundfile as sf\n",
    "import os\n",
    "from io import BytesIO\n",
    "import keyboard\n",
    "from groq import Groq\n",
    "import time\n",
    "import pyaudio\n",
    "import wave\n",
    "import tempfile\n",
    "import pyperclip\n",
    "import webrtcvad\n",
    "import threading\n",
    "from transformers import pipeline, logging\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing local transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whispercpp import Whisper\n",
    "whisper_model = Whisper.from_pretrained(\"base.en\")  # Note method change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whisper model (tiny.en for low latency)\n",
    "whisper_model = Whisper(\"tiny.en\")  # Replace with \"base.en\" or larger models if needed\n",
    "\n",
    "# Configuration\n",
    "SAMPLERATE = 16000  # Whisper works best at this rate\n",
    "BLOCKSIZE = 1024    # 64ms chunks (tweak for lower latency)\n",
    "DEVICE_ID = 1      # Replace with your virtual cable device index (use sd.query_devices())\n",
    "audio_queue = queue.Queue(maxsize=10)  # Thread-safe buffer for audio chunks\n",
    "\n",
    "# Callback function to capture audio chunks\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"Non-blocking audio chunk handler.\"\"\"\n",
    "    if status:\n",
    "        print(f\"Audio callback error: {status}\")\n",
    "    audio_queue.put(indata[:, 0].copy())  # Convert to mono and add to queue\n",
    "\n",
    "# Process audio chunks in real-time using Whisper\n",
    "def process_audio():\n",
    "    \"\"\"Continuously process audio chunks from the queue.\"\"\"\n",
    "    print(\"Starting transcription...\")\n",
    "    while True:\n",
    "        try:\n",
    "            # Get an audio chunk from the queue\n",
    "            chunk = audio_queue.get()\n",
    "            # Transcribe the chunk using Whisper.cpp\n",
    "            text = whisper_model.transcribe(chunk.astype(np.float32))\n",
    "            if text.strip():  # If transcription is not empty\n",
    "                print(f\"Transcribed Text: {text}\")\n",
    "                # Example: Trigger response for specific keywords\n",
    "                if \"memoization\" in text.lower():\n",
    "                    print(\"[Response] Memoization is an optimization technique...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription: {e}\")\n",
    "\n",
    "# Start the real-time audio stream and transcription loop\n",
    "def start_real_time_transcription():\n",
    "    \"\"\"Start the audio stream and transcription.\"\"\"\n",
    "    try:\n",
    "        print(\"Initializing audio stream...\")\n",
    "        with sd.InputStream(samplerate=SAMPLERATE, blocksize=BLOCKSIZE,\n",
    "                            device=DEVICE_ID, channels=1, callback=audio_callback):\n",
    "            process_audio()  # Start processing audio chunks in real-time\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing audio stream: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time meeting assistant...\")\n",
    "    start_real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing online API transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio recording parameters\n",
    "CHUNK = 480\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "VAD_AGGRESSIVENESS = 3  # 1-3 (3=most aggressive noise filtering)\n",
    "MIN_SPEECH_DURATION = 1.2  # Seconds of speech to trigger processing\n",
    "PRE_SPEECH_BUFFER = 2.0  # Seconds to capture before speech starts\n",
    "POST_SPEECH_BUFFER = 1.5  # Seconds to capture after speech ends\n",
    "MAX_BUFFER_SECONDS = 5 # Maximum buffer size in seconds\n",
    "\n",
    "vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)  # Aggressive noise filtering\n",
    "audio_queue = queue.Queue()\n",
    "processing_lock = threading.Lock()\n",
    "in_speech = False\n",
    "speech_start = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_lock = threading.Lock()\n",
    "audio_buffer = np.array([], dtype=np.int16)  # Declare and initialize audio_buffer globally\n",
    "in_speech = False\n",
    "speech_start = 0\n",
    "buffer_start_idx = 0\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"VAD based audio recording with threading.\"\"\"\n",
    "    global audio_buffer, in_speech, speech_start, buffer_start_idx\n",
    "    \n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        global audio_buffer, in_speech, speech_start, buffer_start_idx\n",
    "        \n",
    "        pcm = np.frombuffer(in_data, dtype=np.int16)\n",
    "\n",
    "        with processing_lock:\n",
    "            is_speech = vad.is_speech(pcm.tobytes(), RATE, len(pcm))\n",
    "            \n",
    "            if is_speech:\n",
    "                if not in_speech:\n",
    "                    # Speech start: track buffer position\n",
    "                    speech_start = time.time()\n",
    "                    buffer_start_idx = max(0, len(audio_buffer) - RATE//2)  # 0.5s lookback\n",
    "                    in_speech = True\n",
    "                audio_buffer = np.concatenate([audio_buffer, pcm])\n",
    "            else:\n",
    "                if in_speech:\n",
    "                    # Speech end: calculate captured duration\n",
    "                    captured_seconds = (len(audio_buffer) - buffer_start_idx) / RATE\n",
    "                    if captured_seconds >= MIN_SPEECH_DURATION:\n",
    "                        # Capture pre/post speech context\n",
    "                        pre_samples = min(buffer_start_idx, int(RATE * PRE_SPEECH_BUFFER))\n",
    "                        post_samples = int(RATE * POST_SPEECH_BUFFER)\n",
    "                        \n",
    "                        segment = audio_buffer[\n",
    "                            buffer_start_idx - pre_samples : \n",
    "                            len(audio_buffer) + post_samples\n",
    "                        ]\n",
    "                        audio_queue.put(segment.copy())\n",
    "                        \n",
    "                    in_speech = False\n",
    "\n",
    "        print(f\"VAD: {'ðŸ—£ï¸' if is_speech else 'ðŸ”‡'} | Buffer: {len(audio_buffer)/RATE:.1f}s\", end='\\r')\n",
    "        return (None, pyaudio.paContinue)\n",
    "\n",
    "    vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                    input=True, frames_per_buffer=CHUNK,\n",
    "                    stream_callback=callback, start=False)\n",
    "    stream.start_stream()\n",
    "    return stream\n",
    "\n",
    "def save_audio(audio_numpy):\n",
    "    \"\"\"Save numpy array directly\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
    "        sf.write(f, audio_numpy, RATE, format='WAV', subtype='PCM_16')\n",
    "        return f.name\n",
    "\n",
    "def transcribe_audio(audio_numpy):\n",
    "    \"\"\"Direct memory-based transcription\"\"\"\n",
    "    # Convert numpy array to bytes buffer\n",
    "    buffer = BytesIO()\n",
    "    audio_numpy = audio_numpy.astype(np.float32) / 32768.0 # Normalize to float32\n",
    "    sf.write(buffer, audio_numpy, RATE, format='WAV', \n",
    "             subtype='FLOAT')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    try:\n",
    "        response = client.audio.transcriptions.create(\n",
    "            file=(\"audio.wav\", buffer.read()),\n",
    "            model=\"whisper-large-v3-turbo\",  # Verified correct model name\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Transcription failed: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def play_audio(filename=\"recorded_audio.wav\"):\n",
    "    import pyaudio\n",
    "    import wave\n",
    "\n",
    "    wf = wave.open(filename, 'rb')\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream for playback\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True)\n",
    "\n",
    "    # Read and play back data in chunks\n",
    "    chunk = 1024\n",
    "    data = wf.readframes(chunk)\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunk)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    print(\"Playback finished.\")\n",
    "\n",
    "def process_transcription(text):\n",
    "    \"\"\"Process the transcribed text and generate a response.\"\"\"\n",
    "    cleaned = text.lower().strip()[:200] # Limit input length\n",
    "    if not cleaned:\n",
    "        return \"No speech detected\"\n",
    "    keywords = {\n",
    "        \"memoization\": \"Memoization is an optimization technique that stores results of expensive function calls.\",\n",
    "        \"recursion\": \"Recursion is a method where a function calls itself to solve smaller instances of a problem.\",\n",
    "        \"dynamic programming\": \"Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems.\",\n",
    "        \"algorithm\": \"An algorithm is a step-by-step procedure for calculations.\",\n",
    "        \"data structure\": \"A data structure is a particular way of organizing and storing data in a computer.\",\n",
    "        \"machine learning\": \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "        \"artificial intelligence\": \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "        \"deep learning\": \"Deep learning is a subset of machine learning that uses neural networks with many layers.\",\n",
    "        \"natural language processing\": \"Natural language processing is a field of AI that focuses on the interaction between computers and humans through natural language.\",\n",
    "        \"computer vision\": \"Computer vision is a field of AI that enables computers to interpret and understand visual information from the world.\",\n",
    "        \"reinforcement learning\": \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.\",\n",
    "        \"supervised learning\": \"Supervised learning is a type of machine learning where the model is trained on labeled data.\",\n",
    "        \"unsupervised learning\": \"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data.\",\n",
    "        \"transfer learning\": \"Transfer learning is a technique where a model developed for one task is reused as the starting point for a model on a second task.\",\n",
    "        \"chini\": \"Chini to pagal hai.\"\n",
    "    }\n",
    "    \n",
    "    for kw in sorted(keywords.keys(), key=len, reverse=True):\n",
    "        if kw in cleaned:\n",
    "            return keywords[kw]\n",
    "    return f\"Command not recognized: {cleaned[:30]}...\"\n",
    "\n",
    "def copy_to_clipboard(text):\n",
    "    \"\"\"Copy text to clipboard.\"\"\"\n",
    "    pyperclip.copy(text)\n",
    "    print(\"Response copied to clipboard!\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     while True:\n",
    "#         input(\"Press Enter to start recording...\")\n",
    "#         audio_frames = record_audio()\n",
    "#         temp_file_path = save_audio(audio_frames)\n",
    "#         transcription = transcribe_audio(temp_file_path)\n",
    "#         print(\"Transcription:\", transcription)\n",
    "#         os.remove(temp_file_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = record_audio()\n",
    "print(\"Recording started. Press 'q' to stop.\")\n",
    "while True:\n",
    "    if keyboard.is_pressed('q'):\n",
    "        print(\"Recording stopped.\")\n",
    "        break\n",
    "    audio_data = audio_queue.get(timeout=0.2)\n",
    "    if audio_data.size >= int(RATE * 0.3):\n",
    "        # Process the audio data\n",
    "        transcription = transcribe_audio(audio_data)\n",
    "        print(\"Transcription:\", transcription)\n",
    "    else:\n",
    "        print(\"No audio data received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()  # Suppress warnings\n",
    "\n",
    "class QuestionDetector:\n",
    "    def __init__(self):\n",
    "        # Lightweight model for real-time use (90MB)\n",
    "        self.classifier = pipeline(\n",
    "            \"text-classification\", \n",
    "            model=\"shahrukhx01/question-vs-statement-classifier\"\n",
    "        )\n",
    "\n",
    "        self.client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "        self.model = \"llama3-70b-8192\"\n",
    "        \n",
    "    def detect_question(self, text):\n",
    "        \"\"\"Returns True if input text is a question\"\"\"\n",
    "        if not text.strip():\n",
    "            return False\n",
    "            \n",
    "        result = self.classifier(text[:512])  # Trim to model's max length\n",
    "        return result[0]['label'] == 'LABEL_1' and result[0]['score'] > 0.85\n",
    "    \n",
    "    def generate_answer(self, question, context=\"\"):\n",
    "        \"\"\"Generate an answer to the question using Groq API\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"system\", \"content\": f\"\"\"You are an expert interview coach. Generate concise, professional responses using:\n",
    "                        - STAR method (Situation, Task, Action, Result)\n",
    "                        - Max 3 sentences\n",
    "                        - Incorporate keywords: {context}\n",
    "                        - Maintain a professional tone\"\"\"},\n",
    "                          {\"role\": \"user\", \"content\": f\"Suggest response to: {question}\"}],\n",
    "                model=self.model,\n",
    "                temperature=0.2,\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return \"Sorry, I couldn't generate an answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"What is your greatest strength?\", \n",
    "    \"Tell me about a time you solved a problem.\",\n",
    "    \"I worked at XYZ Corp for 5 years.\",\n",
    "    \"Why did you leave your previous job?\",\n",
    "    \"It was great working with my team.\"\n",
    "]\n",
    "context = \"AWS, Python, React, Team Leadership\"\n",
    "\n",
    "qd = QuestionDetector()\n",
    "for text in texts:\n",
    "    print(f\"Text: {text}\")\n",
    "    if qd.detect_question(text):\n",
    "        print(\"Detected as a question.\")\n",
    "        response = qd.generate_answer(text, context)\n",
    "        print(f\"Generated Answer: {response}\\n\")\n",
    "    else:\n",
    "        print(\"Detected as a statement.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤ Listening... (Press Ctrl+C to stop)\n",
      "Text: Hi, nice to meet you.\n",
      "Detected as a statement.\n",
      "Text: Hi, nice to meet you. Today is a great day.\n",
      "Detected as a statement.\n",
      "Text: a great day. So how are you doing?\n",
      "Detected as a question.\n",
      "ðŸ’¬ Generated Response: Here's a possible response:\n",
      "\n",
      "\"In my previous role, I had the opportunity to lead a cross-functional team in developing a cloud-based application on AWS, where we utilized Python for backend development and React for frontend implementation. As the team leader, I was responsible for overseeing the project's progress, ensuring timely delivery, and fostering a collaborative environment. Through effective team leadership, we successfully launched the application, resulting in a 30% increase in customer engagement and a 25% reduction in costs.\"\n",
      "\n",
      "This response uses the STAR method to provide a concise and professional answer, incorporating the required keywords (AWS, Python, React, Team Leadership).\n",
      "\n",
      "Text: So how are you doing? I was thinking\n",
      "Detected as a question.\n",
      "ðŸ’¬ Generated Response: Here's a possible response:\n",
      "\n",
      "\"In my previous role, I had the opportunity to lead a cross-functional team in developing a cloud-based application on AWS, where we utilized Python for backend development and React for frontend implementation. As the team leader, I was responsible for overseeing the project's progress, ensuring timely delivery, and fostering a collaborative environment. Through effective team leadership, we successfully launched the application, resulting in a 30% increase in customer engagement and a 25% reduction in costs.\"\n",
      "\n",
      "This response uses the STAR method to provide a concise and professional answer, incorporating the required keywords (AWS, Python, React, Team Leadership).\n",
      "\n",
      "Text: How are you doing? I was thinking maybe if you could\n",
      "Detected as a question.\n",
      "ðŸ’¬ Generated Response: Here's a possible response:\n",
      "\n",
      "\"In my previous role, I was tasked with leading a cross-functional team to develop a cloud-based application using AWS. I worked closely with my team to design and implement a scalable architecture, leveraging Python for backend development and React for frontend. As a result, we successfully delivered the project ahead of schedule, receiving positive feedback from stakeholders and demonstrating my ability to drive team leadership and collaboration.\"\n",
      "\n",
      "Text: I was thinking maybe if you could tell me about\n",
      "Detected as a statement.\n",
      "Text: Maybe if you could tell me about a person\n",
      "Detected as a statement.\n",
      "Text: tell me about the person or if you could tell me about dynamic\n",
      "Detected as a statement.\n",
      "Text: or if you could tell me about dynamic programming. Tell me how does memorization help\n",
      "Detected as a question.\n",
      "ðŸ’¬ Generated Response: Here's a possible response:\n",
      "\n",
      "\"In my previous role, I had the opportunity to lead a cross-functional team in developing a cloud-based application on AWS, where we utilized Python for backend development and React for frontend implementation. As the team lead, I was responsible for overseeing the project's progress, ensuring timely delivery, and fostering a collaborative environment. Through effective team leadership, we successfully launched the application, resulting in a 30% increase in customer engagement and a 25% reduction in costs.\"\n",
      "\n",
      "This response uses the STAR method to provide a concise and professional answer, incorporating the required keywords (AWS, Python, React, Team Leadership).\n",
      "\n",
      "VAD: ðŸ”‡ | Buffer: 15.9ss\n",
      "ðŸ›‘ Stopping...\n",
      "\n",
      "=== COMPLETE TRANSCRIPT ===\n",
      "1. Hi, nice to meet you.\n",
      "2. Hi, nice to meet you. Today is a great day.\n",
      "3. a great day. So how are you doing?\n",
      "4. So how are you doing? I was thinking\n",
      "5. How are you doing? I was thinking maybe if you could\n",
      "6. I was thinking maybe if you could tell me about\n",
      "7. Maybe if you could tell me about a person\n",
      "8. tell me about the person or if you could tell me about dynamic\n",
      "9. or if you could tell me about dynamic programming. Tell me how does memorization help\n"
     ]
    }
   ],
   "source": [
    "FULL_TRANSCRIPT = []  # Simple list to store all spoken phrases\n",
    "\n",
    "def main():\n",
    "    global FULL_TRANSCRIPT\n",
    "\n",
    "    # Initialize detector\n",
    "    qd = QuestionDetector()\n",
    "    context = \"AWS, Python, React, Team Leadership\"\n",
    "    \n",
    "    stream = record_audio()\n",
    "    print(\"ðŸŽ¤ Listening... (Press Ctrl+C to stop)\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                audio_data = audio_queue.get(timeout=0.2)\n",
    "                \n",
    "                if audio_data.size >= int(RATE * 0.3):  # Minimal audio threshold\n",
    "                    # Core pipeline\n",
    "                    transcription = transcribe_audio(audio_data)\n",
    "                    # response = process_transcription(transcription)\n",
    "                    print(f\"Text: {transcription}\")\n",
    "                    # if question detected\n",
    "                    if qd.detect_question(transcription):\n",
    "                        print(\"Detected as a question.\")\n",
    "                        response = qd.generate_answer(text, context)    \n",
    "                        print(f\"ðŸ’¬ Generated Response: {response}\\n\")\n",
    "                    else:\n",
    "                        print(\"Detected as a statement.\")\n",
    "                        response = \"Sorry, I couldn't generate an answer.\"\n",
    "                    \n",
    "                    # Simple storage\n",
    "                    FULL_TRANSCRIPT.append(transcription)\n",
    "                    \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nðŸ›‘ Stopping...\")\n",
    "        \n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        \n",
    "        # Simple verification\n",
    "        print(\"\\n=== COMPLETE TRANSCRIPT ===\")\n",
    "        for i, phrase in enumerate(FULL_TRANSCRIPT, 1):\n",
    "            print(f\"{i}. {phrase}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
