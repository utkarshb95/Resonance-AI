{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())  # Find virtual cable's index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "duration = 5  # seconds\n",
    "sample_rate = 16000  # Hz\n",
    "\n",
    "# Record audio\n",
    "print(\"Recording...\")\n",
    "audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2, dtype=np.float32)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "print(\"Recording finished.\")\n",
    "\n",
    "# Play the recorded audio\n",
    "print(\"Playing back the recorded audio...\")\n",
    "sd.play(audio_data, samplerate=sample_rate)\n",
    "sd.wait()  # Wait until playback is finished\n",
    "print(\"Playback finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Playing back the recorded audio...\")\n",
    "sd.play(audio_data, samplerate=sample_rate)\n",
    "sd.wait()  # Wait until playback is finished\n",
    "print(\"Playback finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "from collections import deque\n",
    "from whispercpp import Whisper\n",
    "from openai import OpenAI\n",
    "import soundfile as sf\n",
    "import os\n",
    "from io import BytesIO\n",
    "import keyboard\n",
    "from groq import Groq\n",
    "import time\n",
    "import pyaudio\n",
    "import wave\n",
    "import tempfile\n",
    "import pyperclip\n",
    "import webrtcvad\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webrtcvad-wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sounddevice numpy whispercpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing local transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whispercpp import Whisper\n",
    "whisper_model = Whisper.from_pretrained(\"base.en\")  # Note method change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Whisper model (tiny.en for low latency)\n",
    "whisper_model = Whisper(\"tiny.en\")  # Replace with \"base.en\" or larger models if needed\n",
    "\n",
    "# Configuration\n",
    "SAMPLERATE = 16000  # Whisper works best at this rate\n",
    "BLOCKSIZE = 1024    # 64ms chunks (tweak for lower latency)\n",
    "DEVICE_ID = 1      # Replace with your virtual cable device index (use sd.query_devices())\n",
    "audio_queue = queue.Queue(maxsize=10)  # Thread-safe buffer for audio chunks\n",
    "\n",
    "# Callback function to capture audio chunks\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    \"\"\"Non-blocking audio chunk handler.\"\"\"\n",
    "    if status:\n",
    "        print(f\"Audio callback error: {status}\")\n",
    "    audio_queue.put(indata[:, 0].copy())  # Convert to mono and add to queue\n",
    "\n",
    "# Process audio chunks in real-time using Whisper\n",
    "def process_audio():\n",
    "    \"\"\"Continuously process audio chunks from the queue.\"\"\"\n",
    "    print(\"Starting transcription...\")\n",
    "    while True:\n",
    "        try:\n",
    "            # Get an audio chunk from the queue\n",
    "            chunk = audio_queue.get()\n",
    "            # Transcribe the chunk using Whisper.cpp\n",
    "            text = whisper_model.transcribe(chunk.astype(np.float32))\n",
    "            if text.strip():  # If transcription is not empty\n",
    "                print(f\"Transcribed Text: {text}\")\n",
    "                # Example: Trigger response for specific keywords\n",
    "                if \"memoization\" in text.lower():\n",
    "                    print(\"[Response] Memoization is an optimization technique...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during transcription: {e}\")\n",
    "\n",
    "# Start the real-time audio stream and transcription loop\n",
    "def start_real_time_transcription():\n",
    "    \"\"\"Start the audio stream and transcription.\"\"\"\n",
    "    try:\n",
    "        print(\"Initializing audio stream...\")\n",
    "        with sd.InputStream(samplerate=SAMPLERATE, blocksize=BLOCKSIZE,\n",
    "                            device=DEVICE_ID, channels=1, callback=audio_callback):\n",
    "            process_audio()  # Start processing audio chunks in real-time\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing audio stream: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting real-time meeting assistant...\")\n",
    "    start_real_time_transcription()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing online API transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio recording parameters\n",
    "CHUNK = 480\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "VAD_AGGRESSIVENESS = 3  # 1-3 (3=most aggressive noise filtering)\n",
    "MIN_SPEECH_DURATION = 1.2  # Seconds of speech to trigger processing\n",
    "PRE_SPEECH_BUFFER = 2.0  # Seconds to capture before speech starts\n",
    "POST_SPEECH_BUFFER = 1.5  # Seconds to capture after speech ends\n",
    "MAX_BUFFER_SECONDS = 5 # Maximum buffer size in seconds\n",
    "\n",
    "vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)  # Aggressive noise filtering\n",
    "audio_queue = queue.Queue()\n",
    "processing_lock = threading.Lock()\n",
    "in_speech = False\n",
    "speech_start = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_lock = threading.Lock()\n",
    "audio_buffer = np.array([], dtype=np.int16)  # Declare and initialize audio_buffer globally\n",
    "in_speech = False\n",
    "speech_start = 0\n",
    "buffer_start_idx = 0\n",
    "\n",
    "def record_audio():\n",
    "    \"\"\"VAD based audio recording with threading.\"\"\"\n",
    "    global audio_buffer, in_speech, speech_start, buffer_start_idx\n",
    "    \n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        global audio_buffer, in_speech, speech_start, buffer_start_idx\n",
    "        \n",
    "        pcm = np.frombuffer(in_data, dtype=np.int16)\n",
    "\n",
    "        with processing_lock:\n",
    "            is_speech = vad.is_speech(pcm.tobytes(), RATE, len(pcm))\n",
    "            \n",
    "            if is_speech:\n",
    "                if not in_speech:\n",
    "                    # Speech start: track buffer position\n",
    "                    speech_start = time.time()\n",
    "                    buffer_start_idx = max(0, len(audio_buffer) - RATE//2)  # 0.5s lookback\n",
    "                    in_speech = True\n",
    "                audio_buffer = np.concatenate([audio_buffer, pcm])\n",
    "            else:\n",
    "                if in_speech:\n",
    "                    # Speech end: calculate captured duration\n",
    "                    captured_seconds = (len(audio_buffer) - buffer_start_idx) / RATE\n",
    "                    if captured_seconds >= MIN_SPEECH_DURATION:\n",
    "                        # Capture pre/post speech context\n",
    "                        pre_samples = min(buffer_start_idx, int(RATE * PRE_SPEECH_BUFFER))\n",
    "                        post_samples = int(RATE * POST_SPEECH_BUFFER)\n",
    "                        \n",
    "                        segment = audio_buffer[\n",
    "                            buffer_start_idx - pre_samples : \n",
    "                            len(audio_buffer) + post_samples\n",
    "                        ]\n",
    "                        audio_queue.put(segment.copy())\n",
    "                        \n",
    "                    in_speech = False\n",
    "\n",
    "        print(f\"VAD: {'🗣️' if is_speech else '🔇'} | Buffer: {len(audio_buffer)/RATE:.1f}s\", end='\\r')\n",
    "        return (None, pyaudio.paContinue)\n",
    "\n",
    "    vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = p.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                    input=True, frames_per_buffer=CHUNK,\n",
    "                    stream_callback=callback, start=False)\n",
    "    stream.start_stream()\n",
    "    return stream\n",
    "\n",
    "def save_audio(audio_numpy):\n",
    "    \"\"\"Save numpy array directly\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as f:\n",
    "        sf.write(f, audio_numpy, RATE, format='WAV', subtype='PCM_16')\n",
    "        return f.name\n",
    "\n",
    "def transcribe_audio(audio_numpy):\n",
    "    \"\"\"Direct memory-based transcription\"\"\"\n",
    "    # Convert numpy array to bytes buffer\n",
    "    buffer = BytesIO()\n",
    "    audio_numpy = audio_numpy.astype(np.float32) / 32768.0 # Normalize to float32\n",
    "    sf.write(buffer, audio_numpy, RATE, format='WAV', \n",
    "             subtype='FLOAT')\n",
    "    buffer.seek(0)\n",
    "\n",
    "    try:\n",
    "        response = client.audio.transcriptions.create(\n",
    "            file=(\"audio.wav\", buffer.read()),\n",
    "            model=\"whisper-large-v3-turbo\",  # Verified correct model name\n",
    "            response_format=\"text\",\n",
    "            language=\"en\",\n",
    "            temperature=0.2,\n",
    "        )\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Transcription failed: {str(e)}\")\n",
    "        return \"\"\n",
    "\n",
    "def play_audio(filename=\"recorded_audio.wav\"):\n",
    "    import pyaudio\n",
    "    import wave\n",
    "\n",
    "    wf = wave.open(filename, 'rb')\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    # Open stream for playback\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True)\n",
    "\n",
    "    # Read and play back data in chunks\n",
    "    chunk = 1024\n",
    "    data = wf.readframes(chunk)\n",
    "    while data:\n",
    "        stream.write(data)\n",
    "        data = wf.readframes(chunk)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "    print(\"Playback finished.\")\n",
    "\n",
    "def process_transcription(text):\n",
    "    \"\"\"Process the transcribed text and generate a response.\"\"\"\n",
    "    cleaned = text.lower().strip()[:200] # Limit input length\n",
    "    if not cleaned:\n",
    "        return \"No speech detected\"\n",
    "    keywords = {\n",
    "        \"memoization\": \"Memoization is an optimization technique that stores results of expensive function calls.\",\n",
    "        \"recursion\": \"Recursion is a method where a function calls itself to solve smaller instances of a problem.\",\n",
    "        \"dynamic programming\": \"Dynamic programming is a method for solving complex problems by breaking them down into simpler subproblems.\",\n",
    "        \"algorithm\": \"An algorithm is a step-by-step procedure for calculations.\",\n",
    "        \"data structure\": \"A data structure is a particular way of organizing and storing data in a computer.\",\n",
    "        \"machine learning\": \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "        \"artificial intelligence\": \"Artificial intelligence is the simulation of human intelligence in machines.\",\n",
    "        \"deep learning\": \"Deep learning is a subset of machine learning that uses neural networks with many layers.\",\n",
    "        \"natural language processing\": \"Natural language processing is a field of AI that focuses on the interaction between computers and humans through natural language.\",\n",
    "        \"computer vision\": \"Computer vision is a field of AI that enables computers to interpret and understand visual information from the world.\",\n",
    "        \"reinforcement learning\": \"Reinforcement learning is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize cumulative reward.\",\n",
    "        \"supervised learning\": \"Supervised learning is a type of machine learning where the model is trained on labeled data.\",\n",
    "        \"unsupervised learning\": \"Unsupervised learning is a type of machine learning where the model is trained on unlabeled data.\",\n",
    "        \"transfer learning\": \"Transfer learning is a technique where a model developed for one task is reused as the starting point for a model on a second task.\",\n",
    "        \"chini\": \"Chini to pagal hai.\"\n",
    "    }\n",
    "    \n",
    "    for kw in sorted(keywords.keys(), key=len, reverse=True):\n",
    "        if kw in cleaned:\n",
    "            return keywords[kw]\n",
    "    return f\"Command not recognized: {cleaned[:30]}...\"\n",
    "\n",
    "def copy_to_clipboard(text):\n",
    "    \"\"\"Copy text to clipboard.\"\"\"\n",
    "    pyperclip.copy(text)\n",
    "    print(\"Response copied to clipboard!\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     while True:\n",
    "#         input(\"Press Enter to start recording...\")\n",
    "#         audio_frames = record_audio()\n",
    "#         temp_file_path = save_audio(audio_frames)\n",
    "#         transcription = transcribe_audio(temp_file_path)\n",
    "#         print(\"Transcription:\", transcription)\n",
    "#         os.remove(temp_file_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_TRANSCRIPT = []  # Simple list to store all spoken phrases\n",
    "LAST_RESPONSE = \"\"\n",
    "\n",
    "def main():\n",
    "    global FULL_TRANSCRIPT, LAST_RESPONSE\n",
    "    \n",
    "    stream = record_audio()\n",
    "    print(\"🎤 Listening... (Press Ctrl+C to stop)\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                audio_data = audio_queue.get(timeout=0.2)\n",
    "                \n",
    "                if audio_data.size >= int(RATE * 0.3):  # Minimal audio threshold\n",
    "                    # Core pipeline\n",
    "                    transcription = transcribe_audio(audio_data)\n",
    "                    response = process_transcription(transcription)\n",
    "                    \n",
    "                    # Simple storage\n",
    "                    FULL_TRANSCRIPT.append(transcription)\n",
    "                    LAST_RESPONSE = response\n",
    "                    \n",
    "                    # Immediate feedback\n",
    "                    print(f\"\\n📝 Heard: {transcription}\")\n",
    "                    print(f\"💬 Response: {response}\")\n",
    "                    pyperclip.copy(response)\n",
    "                    \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n🛑 Stopping...\")\n",
    "        \n",
    "    finally:\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        \n",
    "        # Simple verification\n",
    "        print(\"\\n=== COMPLETE TRANSCRIPT ===\")\n",
    "        for i, phrase in enumerate(FULL_TRANSCRIPT, 1):\n",
    "            print(f\"{i}. {phrase}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
